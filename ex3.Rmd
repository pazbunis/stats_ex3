---
title: "Ex 3 - fMRI Data"
author: "Paz Bunis & Tal Aviel"
output:
  html_notebook: default
  pdf_document: default
---

```{r}
install.packages('glmnet')
```

```{r}
library('glmnet')
load('wavpyr.RData')
load('stim.RData')
```

```{r}
# Prepare feature attributes:

# generates the feature attributes vector.
# Orientation -> featAtt[1,] (1 vertical, 5 horizontal)  
# Pyramid level -> featAtt[2,] (How big is the feature)
# Vertical location -> featAtt[3,]  (vert/horz might be confused)
# Horizontal location -> featAtt[4,] 

featAtt = function(){
    # featAttVec: row1 - orientation, row2 - pyr-level, row3 - locationx, row4 - locationy
    pyr  = c(1,8,4*8,16*8,64*8,256*8,1024*8)
    lens = c(0,1,2,   4,   8,   16,    32,  64)
    featAttVec = matrix(0,nr=4,nc=sum(pyr))
    featAttVec[1,] = (0:(sum(pyr)-1)) %% 8
    cumsumpyr = cumsum(pyr)
    featAttVec[2,1] = 1;
    featAttVec[3:4,1] = 0;
    for (i in 1:(length(cumsumpyr)-1)) {
        k = 0
        for (j in seq((cumsumpyr[i]+1),cumsumpyr[i+1],8)) {
            featAttVec[2,j:(j+7)] = i+1
            featAttVec[3,j:(j+7)] = floor(k/(lens[i+1])) + 1
            featAttVec[4,j:(j+7)] = (k%%(lens[i+1])) + 1
            k = k + 1
        }
    }
    return(featAttVec)
}
```

## Preparing the Data Sets
```{r}
data_sets = load("fMRIclass.RData")
X=fit_feat
y=fit_data

set.seed(101)
sample <- sample.int(n = nrow(X), size = floor(.75*nrow(X)), replace = F)
X_train <- X[sample, ]
X_test  <- X[-sample, ]
y_train <- y[sample, ]
y_test  <- y[-sample, ]
```

## 1 Training Prediction Models
```{r}
y_hat = matrix(ncol=15, nrow=nrow(val_feat)) # dataset with unknown labels - need to hand in
y_hat_test = matrix(ncol=15, nrow=nrow(X_test))
best_voxel_mse = -1
best_voxel_betahat = -1
best_voxel_num = 1
best_voxel_y_hat = c()
for (voxel in 1:15) {
  fit=glmnet(X_train,y_train[,voxel])
  y_test_hat_all_lambdas=predict(fit, X_test)
  
  mse_per_lambda=colMeans((y_test_hat_all_lambdas-y_test[,voxel])^2)
  
  min_mse = min(mse_per_lambda)
  best_mse_lambda=fit$lambda[which.min(mse_per_lambda)]
  
  betahat=coef(fit,s=best_mse_lambda)
  
  y_hat[, voxel] = predict(fit, val_feat, s=best_mse_lambda)
  y_hat_test[, voxel] = predict(fit, X_test, s=best_mse_lambda)
  
  if (best_voxel_mse == -1 || best_voxel_mse > min_mse) {
    best_voxel_mse = min_mse
    best_voxel_betahat = betahat
    best_voxel_num = voxel
    #best_voxel_y_hat = predict(fit, X_test, s=best_mse_lambda)
  }
}
```
## 2 Analysis of Results
### 2.1 Best Voxel's Model
After we found the best model out of the 15 models, we calculate the importance of the model's features. 
The importance of feature $i$ is defined as $|\hat{\beta}_i|\cdot\text{sd}(X_i)$.
```{r}
feat_sds = apply(X_train, 2, sd)
feat_importance = apply(X_train, 2, sd) * abs(best_voxel_betahat[-1])
sorted = sort(feat_importance, decreasing=T, index.return=T)
N=10
topN = sorted$ix[1:N]

plot(feat_importance, xlab = "Feature Index", ylab = "importance", main = "Feature Importance")
text(feat_importance, labels=1:10921, cex= 0.7, pos=2)
```

We can now look at the model's top $10$ most important features (wavelet filters) combined as one image:  
```{r}
important_feats_img = matrix(0,nrow = 128, ncol = 128)
for (i in 1:N) {
  important_feats_img = important_feats_img + t(matrix(Re(wav.pyr[,topN[i]]), nrow = 128)[128:1,]);
}
image(important_feats_img, col = grey.colors(100), main = "The 10 Most Important Wavelet Filters")
```
### 2.2 Best Feature
In order to understand the relationship between the best feature and the response, we plot the feature's value and the response for all 1750 samples, in addition to a locally weighted scatterplot smoothing line (red). 
```{r}
bestFeature = topN[1]
plot(X[,bestFeature], y[,best_voxel_num], xlab = "Feature Value", ylab = "Response")
lines(loess.smooth(X[,bestFeature], y[,best_voxel_num]), col="red", lty=2, lwd=2)
```
As we can see...


```{r}
plot(X[,7], y[,best_voxel_num])

```

```{r}
cor.test(X[,bestFeature], y[,best_voxel_num], method = "spearman")
cor.test(X[,bestFeature], y[,best_voxel_num], method = "pearson")

```

### 2.3 Overshoot and Undershoot

```{r}
diff = y_test[,best_voxel_num] - y_hat_test[,best_voxel_num]
sorted_diff = sort(diff, decreasing=T, index.return=T)
overshoots = sorted_diff$ix[1:10]
comp = (1:1750)[-sample]

for (i in 1:10) {
  image(t(matrix(stim[comp[overshoots[i]],], nrow = 128)[128:1,]) + t(matrix(Re(wav.pyr[,topN[1]]), nrow = 128)[128:1,]),col = grey.colors(100))
}

```

### 2.4 
